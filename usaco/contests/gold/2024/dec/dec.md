# USACO Gold 2024 Dec

## 1. Cowdependency
Group all cows by label, so $L_i$ denotes the sorted set of all cows with label $i$. A very simple greedy strategy works in $O(N^2)$; assuming we have fixed $x$, go through each $L_i$ and group the maximum prefix into one group. Then delete this group, and continue onwards. This is pretty simply implemented with 2p.

There's two variables at play here, $x$ and $d$. $x$ is the max group size and $d$ is the number of groups. It should be clear that because all groups are disjoint, then for a valid $(x,d)$ pair, we get the relationship $xd\ge{N}$, so they are inversely proportional. Therefore, we can afford just fixing $x$ and $d$ each up to $\lceil\sqrt{N}\rceil$. If we fix $x$, then we are to find the minimum number of groups possible $M$, which we have already figured out with the greedy solution. We can say that $M$ is a candidate for any group size $\ge{x}$. If we fix $d$, then we are to find the minimum value of $x$ such that we have at most $d$ groups with that group size. So, letting this minimum be $S$, binary search on $S$, and we can say $d$ is a candidate for any group size $\ge{S}$. 

If we combine these answers together, then we can get the minimum $d$ for any label for any $x$. Just sum up all the minimum $d$ for a specific $x$ across all labels, and we have an answer for the whole group of cows at that $x$. Putting it all together, the algorithm runs in $O(N\sqrt{N}logN)$. 

## 2. Interstellar Intervals
Simple dp runs in $O(N^2)$, and our goal is to simplify that down to $O(N)$. The dp has two main transitions: we can either paint this node non-white, so transition to $dp[i]$ from $dp[i-2x]$ for any length $x$; also, we can leave it white if $s_i=W$, so transition from $dp[i-1]$.

We can go through each $s_j$ and build an inequality for this $j$ that specifies what intervals are not good. Anything in the union of these inequalities isn't good, and we can use complementary counting to find the amount of good intervals (the reason we aren't finding good intervals directly is because there's no easy way to combine all the inequalities together; you can try it yourself). If $s_j=R$, then a pair $(i,x)$ specifying the endpoint and (half) the length of the interval doesn't work, if $j$ is in the range $[i-x+1,i]$ (because then it would be colored blue). Simplify to $i-x+1\le{j}\le{i}$, and we can say that for a pair $(i,x)$ and some index $j$ satisfying $s_j=R$, if $x\ge{i-j+1}$, then this interval is bad.

Do the same for $s_j=B$. The interval is bad if $j$ is in the range $[i-2x+1,i-x]$, so $i-2x+1\le{j}\le{i-x}$. Simplify down to ${{i-j+1}\over{2}}\le{x}\le{i-j}$.

We have already established that $dp[i]$ transitions from $dp[i-2x]$, so let $t=i-2x$. Substituting this back into our inequality for $s_j=R$, we get that it's impossible to transition from $t$ if $t-i\le{-2(i-j+1)}$, so $t\le{2j-2-i}$. What this means is that if we want to transition from $t$ to $i$, there should exist no index $j$ satisfying $s_j=R$ and $t\le{2j-2-i}$.

Substitute $t$ into the inequality for $s_j=B$, and we get $-2({{i-j+1}\over{2}})\ge{t-i}\ge{-2(i-j)}$, which simplifies to $-2(i-j)\le{t-i}\le{-2({{i-j+1}\over{2}})}$ and thus $2j-i\le{t}\le{j-1}$.

Analyzing the behavior of these inequalities, we can see that for the case $R$, the inequality becomes stricter as $j$ increases. So we only need to consider the maximum $j$ with $s_j=R$. Once we have done this, we can see that as $i$ increases, the right hand side decreases at the same rate, meaning that the amount of valid $t$ increases. Initially, the RHS is $2j-2-i=i-2$ because $i=j$ when we first consider $j$. So if we store a boolean array $m[t]$ which denotes whether we can transition from $t$ or not when only restricted to these $R$ inequalities, then it's as simple as keeping track of the RHS value as a pointer and decrementing it when we increase $i$, then updating the array $m$. 

Finally, we need to analyze the behavior of the $B$ inequalities. The right bound doesn't change, but the left bound decreases when $i$ increases. It suffices to maintain an array of these left bounds and decrement each of them by $1$ when $i$ increases, then update an array $m_2[t]$ which stores whether we can transition from $t$ or not when restricted to $B$ inequalities. Because all pointers decrease at the same rate, we can delete any pointer if it points to an already dealt-with value, meaning that all pointers combined move at most $N$ times. Also, the initial value of the left bound for some $j$ is again when $i=j$, so our inequality simplifies to $i\le{t}\le{i-1}$. Basically, we only need to consider this pointer starting one iteration later, when it reaches $i-1$.

The union of $m$ and $m_2$ stores which values we can't transition from. So when we update any of these arrays, we can simultaneously update the sum of all $dp[t]$ that we can transition from, which we will set $dp[i]$ to be equal to at the end. The result is $dp[N]$, calculated in $O(N)$.

## 3. Job Completion
Possible greedy strategies that come to mind are sorting by $s_i$ and $t_i$. But we can see that $s_i+t_i$ is the maximum possible time that we can finish a job, so we can sort by it. That isn't a good proof however, so the following should solidify this idea. 

For any two jobs $i$ and $j$, if $s_i\le{s_j}$ and $t_i\le{t_j}$ then we obviously do job $i$ before job $j$, and the inequality is satisfied. If job $i$ can be done before $j$ but not the other way around, then letting the time we finished the previous job be $T$, we get that $T+t_i\le{s_j}$ and $T+t_j>{s_i}$. We can see that after rearranging to $s_j-t_i\ge{T}$ and $s_i-t_j<T$, that we can combine to get $s_j-t_i>{s_i-t_j}$. Simplify to $s_j+t_j>s_i+s_j$ to get the resulting inequality. For the final case, if two jobs can be done in either order, then it really doesn't matter which one has a larger $s+t$ value; we can swap the jobs as necessary. For future reference refer to $s_i+t_i$ as $a_i$.

There exists an $O(N^2)$ dp to calculate the answer. Similarly to LIS dp, let $dp[i][l]$ be the minimum time in which we can complete any $l$ jobs out of the first $i$ jobs. We can try adding each job to each length $l$ and seeing if the new answer for length $l+1$ is better than our previous answer for $l+1$.

The simplication to $O(NlogN)$ requires a very clever observation. We can claim that for some fixed $i$, then the set of jobs making up $dp[i][l+1]$ is a superset of the set making up $dp[i][l]$. The reason is that if we can reduce $dp[i][l]$ by adding job $i$ to the set making up $dp[i-1][l-1]$, then we can also reduce $dp[i][l+1]$, by adding the job differing between the sets of $dp[i-1][l]$ and $dp[i-1][l+1]$ to this set $dp[i][l]$. If we could have added it to $dp[i-1][l]$, then we can obviously add to $dp[i][l]$ because it has a smaller value.

So for a fixed $i$, the set for the maximum possible $l$, denoted $L$, is a superset of all other sets. Let that set be $S_i$. The implications of this are the following. First, if all $dp[i][l]$ are subsets of $S_i$ (and therefore derived from it), then we only care about storing $S_i$ (i.e., it validates the existence of a greedy strategy). We can also see that $S_{i-1}$ and $S_i$ differ by at most one element â€” this must be the case as $S_{i}$ is derived from either the sets making up $dp[i-1][L-1]$ or $dp[i-1][L]$, and the first is a subset of the second, having one less element. So we can see that the new goal is to find each set $S_i$ by transitioning from $S_{i-1}$ to $S_i$, and, at the end, return $|S_N|$.

Now, if $|S_{i-1}|<|S_i|$, then that extra element is job $i$. If $|S_{i-1}|=|S_i|$, then we effectively have tried adding job $i$ to $S_{i-1}$ to get our set $S_i$, but it caused a contradiction as ${\sum_{j\in{S_i}}}t_j\le{a_i}$. In which case, we have greedily removed the element with the maximum $t_j$ from set $S_i$, and thus $|S_{i-1}|=|S_i|$. Removing the max $t_j$ reduces the most time, and we claim that at most one removal is necessary. This is because we're given freedom to remove job $i$, and if we don't remove job $i$, then we know that the inequality for $S_i$ builds on the one for $S_{i-1}$, except we decrease the LHS (we subtract $t_j-t_i$) and increase the RHS (we process $a_i$ in increasing order). Obviously, it should hold true because the inequality for $S_{i-1}$ held true already.

The algorithm is summarized by the following: store the current set $S_i$. The base case is $S_0=\set{}$. Then for each $i$, try adding it to the previous set $S_{i-1}$. If you can add it, then leave it there. But if there's an issue, remove the maximum $t_j$ from the set, which can be done with a priority queue. Therefore, we get a solution of complexity $O(NlogN)$ per test case.

## Comments
I solved problem $1$ pretty smoothly until I got TLE on test cases 10 and 11, and spent $30$ minutes coming up with alternate ideas when in fact it was actually a very simple fix. The idea of inversely proportional variables seems to be pretty common at this level because I have already bumped into two other problems like this. A very similar problem is Codeforces [2035E - Monster](https://codeforces.com/problemset/problem/2035/E) and less similarly, RMI 2017 - [Hangman 2](https://csacademy.com/contest/rmi-2017-day-1/task/hangman2/). 

Problem $2$: This is ridiculously difficult (probably like 2500 codeforces; my brainstorm is literally as long as the code itself. It's considerably harder than problems like Bovine Genetics); no wonder most people just went with the easy brute-force during the contest. I would've too, I needed extensive confirmation with my inequalities in subtask 3. However, it was pretty easy to generalize everything after that, because my very first attempts at the problem already involved analyzing the behavior of (wrong) inequalities. Coming up with the inequalities themselves aren't difficult, but the most difficult part is generalizing $t=i-2x$ and substituting that into what you already have. Initially, I just wanted to deal with $x$ and then map it to $t$ after going through everything else. As a side note, the third full credit solution in the editorial is pretty similar to my initial thought process.

Job Completion is interesting because in retrospect, the logical steps aren't that difficult to see. You probably could've jumped straight to the greedy strategy but it would be very difficult to see that you can remove the maximum $t_j$ without having the superset insight.

I rank the problem difficulties in order 1,3,2 from easiest to hardest. I struggled a lot on problem 3 for some reason. 